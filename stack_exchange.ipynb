{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- MSE -- \n",
      "tf gradient tape: [0.00666667 0.22666667 0.01333333]\n",
      "mse.gradient: [0.00666667 0.22666667 0.01333333]\n",
      "\n",
      "-- BCE --\n",
      "tf gradient tape: [ 0.         -0.40012383  4.97895166  0.25000067]\n",
      "bce.gradient: [ 1.00000001 -1.60049558 19.91584631  1.00000276]\n",
      "ml master deriv bce: [1.00000000835839, -1.600495578812266, 19.915846312255084, 1.000002760772572]\n"
     ]
    }
   ],
   "source": [
    "# Stack overflow\n",
    "from typing import Optional, Union\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.losses import binary_crossentropy, mean_squared_error\n",
    "import tensorflow as tf\n",
    "\n",
    "def deriv_bce(y, y_hat):\n",
    "    if y == 1:\n",
    "        return -1 / y_hat\n",
    "    else:\n",
    "        return 1 / (1 - y_hat)\n",
    "\n",
    "class MeanSquaredError:\n",
    "    \"\"\"Mean squared error cost (loss) function.\n",
    "\n",
    "    The predictions are the activations of the network. The order of\n",
    "    arguments in the `derivative` was based on\n",
    "    `Four fundamental equations behind backpropagation` from\n",
    "    Nielsen (Ch.2, 2015). Similarly, the gradient calculation in BP1a of \n",
    "    is described in the same resource.\n",
    "    \"\"\"\n",
    "\n",
    "    def gradient(\n",
    "            self, inputs: tuple[np.ndarray, np.ndarray]) -> np.ndarray:\n",
    "        \"\"\"Computes the gradient with respect to all activations (preds).\n",
    "\n",
    "        This is a vectorized function and is called on each element of \n",
    "        an activation vector in order to compute the partial derivative\n",
    "        of the cost with respect to the j^{th} activation for the \n",
    "        l^{th} layer.\n",
    "\n",
    "        MSE = (1/dims) * (pred - true)^{2}\n",
    "        dMSE/dPred =  (2/dim) * (pred - true)\n",
    "\n",
    "        Args:\n",
    "            inputs: Targets, predictions vectors.\n",
    "\n",
    "        Returns:\n",
    "            Vector (gradient) of values.\n",
    "        \"\"\"\n",
    "\n",
    "        targets, predictions = inputs\n",
    "        return (2 / targets.shape[-1]) * (predictions - targets)\n",
    "\n",
    "    def __call__(\n",
    "            self,\n",
    "            inputs: tuple[np.ndarray, np.ndarray],\n",
    "            axis: Optional[int] = None) -> np.float64:\n",
    "        \"\"\"Compute cost given inputs.\n",
    "\n",
    "        Args:\n",
    "            inputs: Targets and predictions vectors.\n",
    "\n",
    "        Return:\n",
    "            Scalar cost.\n",
    "        \"\"\"\n",
    "\n",
    "        targets, predictions = inputs\n",
    "        return np.mean(np.square(targets - predictions), axis=axis)\n",
    "\n",
    "class BinaryCrossEntropy:\n",
    "    \"\"\"Binary cross entropy loss (cost) function.\"\"\"\n",
    "\n",
    "    def __init__(self, from_logits: bool = False):\n",
    "        \"\"\"Initializes sigmoid function for binary cross entropy.\n",
    "\n",
    "        Args:\n",
    "         from_logits: True for logits, false for normalized log \n",
    "                probabilities (i.e., used sigmoid activation function).\n",
    "                Assumes not from logits.\n",
    "        \"\"\"\n",
    "\n",
    "        self.sigmoid = lambda t: 1 / (1 + np.exp(-t))\n",
    "        self.from_logits = from_logits\n",
    "\n",
    "    def gradient(self, inputs: tuple[np.ndarray, np.ndarray]) -> np.ndarray:\n",
    "        \"\"\"Derivative with respect to a single activation (same as derivative).\n",
    "\n",
    "        Should there be a from logits check here??\n",
    "\n",
    "        Args:\n",
    "            inputs: Targets, predictions vectors. Presumably, the inputs \n",
    "            here also have to be normalized log probabilities.\n",
    "\n",
    "        Returns:\n",
    "            Vector (gradient) of values.\n",
    "        \"\"\"\n",
    "        targets, predictions = inputs\n",
    "\n",
    "        if self.from_logits:\n",
    "            predictions = self.sigmoid(predictions)\n",
    "\n",
    "        return -1 * ((targets/predictions) - ((1-targets) / (1-predictions)))\n",
    "\n",
    "    def __call__(self,\n",
    "                 inputs: tuple[np.ndarray, np.ndarray],\n",
    "                 axis: Optional[int] = None) -> np.ndarray:\n",
    "        \"\"\"Compute cost given inputs.\n",
    "\n",
    "        Args:\n",
    "            inputs: Targets and predictions vectors. \n",
    "                Assumes predictions are not from logits.\n",
    "\n",
    "        Return:\n",
    "            Scalar cost.\n",
    "        \"\"\"\n",
    "\n",
    "        targets, predictions = inputs\n",
    "\n",
    "        if self.from_logits:\n",
    "            predictions = self.sigmoid(predictions)\n",
    "\n",
    "        return -1 * np.mean(targets * np.log(predictions) + (1 - targets) * np.log(1 - predictions), axis=axis)\n",
    "\n",
    "# MSE gradient example\n",
    "\n",
    "# Instantiate cost function objects\n",
    "mse = MeanSquaredError()\n",
    "bce = BinaryCrossEntropy()\n",
    "sigmoid = lambda t: 1 / (1 + np.exp(-t))\n",
    "\n",
    "# Validate MSE grad\n",
    "a_L_np = np.array([0.12, 0.35, 0.61])\n",
    "y_true_np = np.array([0.11, 0.01, 0.59])\n",
    "a_L_tf = tf.Variable(a_L_np)\n",
    "y_true_tf = tf.constant(y_true_np)\n",
    "\n",
    "# tf gradient context\n",
    "with tf.GradientTape() as tape:\n",
    "    C = mean_squared_error(y_true=y_true_tf, y_pred=a_L_tf)\n",
    "\n",
    "dC_daL = tape.gradient(C, a_L_tf)\n",
    "print('-- MSE -- ')\n",
    "print('tf gradient tape:', dC_daL.numpy())\n",
    "\n",
    "# My implementation\n",
    "dC_daL_np = mse.gradient((y_true_np, a_L_np))\n",
    "print('mse.gradient:', dC_daL_np)\n",
    "print()\n",
    "\n",
    "#### BCE ####\n",
    "y_true = tf.constant(np.array([0., 1., 0., 0.]))\n",
    "y_pred_logits = np.array([-18.6, 0.51, 2.94, -12.8])\n",
    "y_pred_proba = tf.Variable(sigmoid(y_pred_logits))\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    C = binary_crossentropy(y_true, y_pred_proba)\n",
    "\n",
    "print('-- BCE --')\n",
    "dC_dProbaActivation = tape.gradient(C, y_pred_proba)\n",
    "print('tf gradient tape:', dC_dProbaActivation.numpy())\n",
    "dC_dProbaActivationMine = bce.gradient((y_true, y_pred_proba))\n",
    "print('bce.gradient:', dC_dProbaActivationMine.numpy())\n",
    "print('ml master deriv bce:', [deriv_bce(y_true[sample].numpy(), y_pred_proba[sample].numpy()) for sample in range(y_true.shape[0])])\n",
    "\n",
    "#### Outputs ####\n",
    "# -- MSE -- \n",
    "# tf gradient tape: [0.00666667 0.22666667 0.01333333]\n",
    "# mse.gradient: [0.00666667 0.22666667 0.01333333]\n",
    "\n",
    "# -- BCE --\n",
    "# tf gradient tape: [ 0.         -0.40012383  4.97895166  0.25000067]\n",
    "# bce.gradient: [ 1.00000001 -1.60049558 19.91584631  1.00000276]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "07f0aceed467d0cbae7569fecdfaec78063cba8e23481668ef588a8de6eba6e0"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('blue-mars': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
