{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single layer neural network\n",
    "# Layer 1       Layer 2        Layer 3\n",
    "# O               O              \n",
    "# O               O              O \n",
    "# O               O              O\n",
    "#                 O\n",
    "# w_jk^{1}       w_jk^{2}       w_jk^{3}\n",
    "# b_j^{1}        b_j^{2}        b_j^{3}\n",
    "# j^th neuron in current `l`\n",
    "# k^th neuron in `l-1`\n",
    "# l = {1, 2, 3}\n",
    "# a_j^{l} is the activation for the j^th neuron in the ^lth layer\n",
    "\n",
    "import numpy as np\n",
    "from ops import ReLU, Linear, MeanSquaredError\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w2 (4, 3) DOT a1 (3,) + b2 (4,) = (4,)\n",
      "w3 (2, 4) DOT a2 (4,) = \n",
      "a3: (2,)\n",
      "a2: (4,)\n",
      "z3: (2,)\n",
      "z2: (4,)\n",
      "w3: (2, 4)\n",
      "Is w2 used at all for calculations?\n"
     ]
    }
   ],
   "source": [
    "# Dimensions\n",
    "j1 = 3\n",
    "\n",
    "j2 = 4\n",
    "k2 = j1\n",
    "\n",
    "j3 = 2\n",
    "k3 = j2\n",
    "\n",
    "# Functions\n",
    "relu = ReLU()\n",
    "linear = Linear()\n",
    "cost = MeanSquaredError()\n",
    "\n",
    "# Input vector\n",
    "num_features = 1\n",
    "x = np.random.normal(size=(j1, ))\n",
    "y = np.random.normal(size=(j3,))\n",
    "\n",
    "# Layer components\n",
    "w1 = None\n",
    "b1 = None\n",
    "z1 = None\n",
    "a1 = linear(x)\n",
    "\n",
    "w2 = np.random.uniform(size=(j2, k2))\n",
    "b2 = np.zeros(shape=(j2, ))\n",
    "z2 = np.dot(w2, a1) + b2\n",
    "a2 = relu(z2)\n",
    "print(f'w2 {w2.shape} DOT a1 {a1.shape} + b2 {b2.shape} = {np.dot(w2, a1).shape}')\n",
    "\n",
    "w3 = np.random.uniform(size=(j3, k3))\n",
    "b3 = np.zeros(shape=(j3, ))\n",
    "print(f'w3 {w3.shape} DOT a2 {a2.shape} = ')\n",
    "z3 = np.dot(w3, a2) + b3\n",
    "a3 = linear(z3)\n",
    "\n",
    "print('a3:', a3.shape)\n",
    "print('a2:', a2.shape)\n",
    "\n",
    "print('z3:', z3.shape)\n",
    "print('z2:', z2.shape)\n",
    "\n",
    "print('w3:', w3.shape)\n",
    "\n",
    "print('Is w2 used at all for calculations?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bp1(y_true, a_L, z_L, activation):\n",
    "    \"\"\"delta^{L} = dC/db^{L}\"\"\"\n",
    "    return cost.gradient((y_true, a_L)) * activation.derivative(z_L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2,)\n"
     ]
    }
   ],
   "source": [
    "delta_L = bp1(\n",
    "    y_true=y, a_L=a3, z_L=z3, activation=linear)\n",
    "\n",
    "print(delta_L.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bp2(w_lyr_plus_one, delta_lyr_plus_1, activation, z_lyr):\n",
    "    \"\"\"delta^{l} = dCdb^{l}\"\"\"\n",
    "    return (\n",
    "        np.dot(np.transpose(w_lyr_plus_one), delta_lyr_plus_1) \\\n",
    "        * activation.derivative(z_lyr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4,)\n"
     ]
    }
   ],
   "source": [
    "delta_2 = bp2(\n",
    "    w_lyr_plus_one=w3,\n",
    "    delta_lyr_plus_1=delta_L,\n",
    "    activation=relu,\n",
    "    z_lyr=z2)\n",
    "    \n",
    "print(delta_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bp4(delta_lyr, a_lyr_minus_1):\n",
    "    \"\"\"dC/dW^{l}... based on backprop algo. 3. grad descent.\"\"\"\n",
    "    return np.dot(np.expand_dims(delta_lyr, axis=-1), np.transpose(np.expand_dims(a_lyr_minus_1, axis=-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 4)\n"
     ]
    }
   ],
   "source": [
    "dC_dW_L = bp4(delta_lyr=delta_L, a_lyr_minus_1=a2)\n",
    "print(dC_dW_L.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "07f0aceed467d0cbae7569fecdfaec78063cba8e23481668ef588a8de6eba6e0"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('blue-mars': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
