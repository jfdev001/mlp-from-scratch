% Some notation taken from 
% https://cs230.stanford.edu/files/Notation.pdf

% On variables, parameters, and arguments
% https://math.stackexchange.com/questions/2113138/what-is-the-difference-between-variable-argument-and-parameter

\documentclass{article}
\usepackage[sorting=none]{biblatex}
\addbibresource{sources.bib}
\usepackage{amsmath}

% Titular information
% https://en.wikibooks.org/wiki/LaTeX/Title_Creation
\title{
	An Undergraduate's Explanation of the Multilayer Perceptron: 
	Mathematical Concepts and a Python3 Implementation}
\date{2022 \\ January}
\author{Jared Frazier\thanks{Department of Computer Science (2021-2022), 
Middle Tennessee State University} \thanks{Not endorsed by or affiliated with any of the 
authors or entities associated with references}}
\begin{document}
\maketitle

\section{Preamble}
\quad The purpose of the present document is to explain and implement the major mathematical
constructs/concepts behind feedforward neural networks, specifically the multilayer perceptron.
This includes the layers that compose such networks, the cost (aka loss, error, or objective) function
and activation functions, the forward pass through the network,
the computation of gradients via backpropagation (a concept that is often "handwaved" to the extreme
or explained in so much detail as to be utterly confusing--at least in the author's experience),
and the update of model parameters via mini-batch stochastic gradient descent.
If the ideas such as \textit{layer} and \textit{backpropagation} are entirely unfamiliar
to you, then the author encourages you to visit texts such as
\textit{Deep Learning} (free, online) \cite{Goodfellow2016},
\textit{Neural Networks and Deep Learning} (free, online) \cite{Nielsen2015},
\textit{Hands-on Machine Learning with Scikit-Learn, TensorFlow and Keras 2ed} (buy) \cite{Geron2020},
and/or \textit{Deep Learning with Python 2ed} (buy) \cite{Chollet2021}. The present document
is not intended to be a comprehensive overview of neural networks nor an extremely
in-depth explanation but rather a document that highlights certain concepts that the
author found confusing or ambiguous when he was learning about neural networks.

The explanations in the present document will essentially alternate between mathematics
and concrete implementations using the Python3 programming language with
the NumPy library. Note that the implementations here
are not intended to be optimal. If you would like optimal implementations, the author
encourages you to use a machine learning API such as TensorFlow or PyTorch, the tutorials
for which will abstract and make easily usable many of the concepts elucidated in
the present document.

\section{Introduction}

\quad The neural network (function approximator) is just a chain of geometric transformations (functions)
each parametrized by a weight matrix $W \in \mathcal{R}^{n_x \times n_h}$ and
a bias vector $b \in \mathcal{R}^{n_h}$ on the input vector $x \in \mathcal{R}^{n_x}$.
With this single sentence the author makes several claims and uses notation that may
or may not be familiar to the reader.
Note that $n_x$ is the input size (i.e., output size of the previous layer in a network with $L$ total layers)
and $n_h$ is the number of hidden units in the current layer. The weights and biases
are collectively known as parameters in the literature. Hence, the use of the
phrase a \textit{function} that is \textit{parametrized} by \textit{weights} and \textit{biases}.
Also, note that the input vector $x$ is a \textit{column vector}, which is important
to understand since vector/matrix operations (dot product, Hadamard (element-wise)
product, addition, etc.) restrict their operands to particular shapes. When
using the term \textit{vector}, the author is always referring to a
\textit{column vector} unless otherwise specified. Also, note that $x \in \mathcal{R}^{n_x}$ indicates
that $x$ is a vector with $n_x$ elements and the $j^{th}$ is a real number.
For example, the below vector $x$ is shown and a common vector operation
known as transposition (converts a \textit{column vector} to a
\textit{row vector} and is denoted with a superscript of $\top$) is also shown.
\begin{align}
	x & = \begin{bmatrix}
		x_{1}  \\
		x_{2}  \\
		x_{3}  \\
		\vdots \\
		x_{(n_x)}
	\end{bmatrix}
	=
	\begin{bmatrix}
		x_{0}  \\
		x_{1}  \\
		x_{2}  \\
		\vdots \\
		x_{(n_{x}-1)}
	\end{bmatrix}
	=
	\begin{bmatrix}
		x_{0} & x_{1} & x_{2} & \cdots & x_{(n_{x}-1)}
	\end{bmatrix}^\top
\end{align}

Many programming languages assign the first element of a vector the index
0; this notation is shown above in addition to the more standard
mathematical notation where the first element begins with the index 1.
For the remainder of this document, the author will use the index 0 assumption
since the author's implementation of the the neural network will use the Python
programming language. If you wish to implement the same algorithms in a language
such as R or Wolfram Mathematica, be wary of this index discrepancy.

\section{Equations}
\quad Here the author defines the operations that occur for a multilayer perceptron (MLP). Note
that the MLP can sometimes refer to any class of feedforward
neural network, that is a network that applies affine transformations and
activation functions to input from a previous layer in the network.

% Neural Network Definition
\subsection{Single Hidden Layer Neural Network}
\begin{equation}
	\begin{aligned}
		NeuralNet_{\theta}(X) & =
		\sigma(ReLU(XW^{[1]}
		+ {b}^{[1]})W^{[2]} + {b}^{[2]})                                                                  \\
		                      & = \sigma(g(ReLU(w(X))))                                                   \\
		A^{L}                 & = NeuralNet_{\theta}(X) & \text{Activation matrix $A$ for last layer $L$}
	\end{aligned}
\end{equation}

where $\sigma$, $ReLU$, $g$, and $w$ are defined as follows:

\begin{equation}
	\begin{aligned}
		\sigma(t)           & = \frac{1}{1 + e^{-t}}                                                         \\
		ReLU(t)             & = max(0, t)                                                                    \\
		g_{\theta^{[2]}}(A) & = AW^{[2]} + {b}^{[2]}                                                         \\
		w_{\theta^{[1]}}(A) & = AW^{[1]} + {b}^{[1]}                                                         \\
		u_{\theta^{[l]}}(A) & = AW^{[l]} + b^{[l]}   & \text{General form of $g$ and $w$ for $l^{th}$ layer}
	\end{aligned}
\end{equation}

\subsection{Neural Network Prediction (Forward Pass)}
% Forward Pass
\begin{align}
	\hat{y} & \gets NeuralNet_\theta(X)
\end{align}

% Loss function
\subsection{Mean Squared Error Loss Function}
\begin{equation}
	\begin{aligned}
		\mathcal{L}_\theta(X) & =
		\frac{1}{N} \sum_{i=1}^{N}{ (\hat{y}^{(i)} - y^{(i)} )^{2}}                                                                                                            \\
		                      & = \frac{1}{N} \sum_{i=1}^{N}{(a^{(i)} + y^{(i)})^{2}} & \text{$a^{(i)}$ is the activation vector of the last layer $L$ for the $i^{th}$ input}
	\end{aligned}
\end{equation}

\subsection{Gradient Update}
% Weight updates
\begin{align}
	\theta_i & \gets \theta_i - \eta (\nabla_\theta \mathcal{L}_{\theta}(\hat{y}, y))
\end{align}

\printbibliography

\end{document}