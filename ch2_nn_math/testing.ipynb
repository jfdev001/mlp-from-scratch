{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2ab6f989c10>]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from typing import Tuple, Optional, Union\n",
    "\n",
    "from tensorflow.keras.losses import binary_crossentropy, mean_squared_error\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from mlp import DenseLayer, MLP\n",
    "from ops import ReLU, MeanSquaredError, BinaryCrossEntropy, Sigmoid\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deriv_bce(y, y_hat):\n",
    "    if y == 1:\n",
    "        return -1 / y_hat\n",
    "    else:\n",
    "        return 1 / (1 - y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- MSE -- \n",
      "tf gradient tape: [0.00666667 0.22666667 0.01333333]\n",
      "mse.gradient: [0.00666667 0.22666667 0.01333333]\n",
      "\n",
      "-- BCE --\n",
      "tf gradient tape: [ 0.         -0.40012383  4.97895166  0.25000067]\n",
      "bce.gradient: [ 1.00000001 -1.60049558 19.91584631  1.00000276]\n",
      "ml master deriv bce: [1.00000000835839, -1.600495578812266, 19.915846312255084, 1.000002760772572]\n"
     ]
    }
   ],
   "source": [
    "# Stack overflow\n",
    "from typing import Optional, Union\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.losses import binary_crossentropy, mean_squared_error\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "class MeanSquaredError:\n",
    "    \"\"\"Mean squared error cost (loss) function.\n",
    "\n",
    "    The predictions are the activations of the network. The order of\n",
    "    arguments in the `derivative` was based on\n",
    "    `Four fundamental equations behind backpropagation` from\n",
    "    Nielsen (Ch.2, 2015). Similarly, the gradient calculation in BP1a of \n",
    "    is described in the same resource.\n",
    "    \"\"\"\n",
    "\n",
    "    def gradient(\n",
    "            self, inputs: tuple[np.ndarray, np.ndarray]) -> np.ndarray:\n",
    "        \"\"\"Computes the gradient with respect to all activations (preds).\n",
    "\n",
    "        This is a vectorized function and is called on each element of \n",
    "        an activation vector in order to compute the partial derivative\n",
    "        of the cost with respect to the j^{th} activation for the \n",
    "        l^{th} layer.\n",
    "\n",
    "        MSE = (1/dims) * (pred - true)^{2}\n",
    "        dMSE/dPred =  (2/dim) * (pred - true)\n",
    "\n",
    "        Args:\n",
    "            inputs: Targets, predictions vectors.\n",
    "\n",
    "        Returns:\n",
    "            Vector (gradient) of values.\n",
    "        \"\"\"\n",
    "\n",
    "        targets, predictions = inputs\n",
    "        return (2 / targets.shape[-1]) * (predictions - targets)\n",
    "\n",
    "    def __call__(\n",
    "            self,\n",
    "            inputs: tuple[np.ndarray, np.ndarray],\n",
    "            axis: Optional[int] = None) -> np.float64:\n",
    "        \"\"\"Compute cost given inputs.\n",
    "\n",
    "        Args:\n",
    "            inputs: Targets and predictions vectors.\n",
    "\n",
    "        Return:\n",
    "            Scalar cost.\n",
    "        \"\"\"\n",
    "\n",
    "        targets, predictions = inputs\n",
    "        return np.mean(np.square(targets - predictions), axis=axis)\n",
    "\n",
    "class BinaryCrossEntropy:\n",
    "    \"\"\"Binary cross entropy loss (cost) function.\"\"\"\n",
    "\n",
    "    def __init__(self, from_logits: bool = False):\n",
    "        \"\"\"Initializes sigmoid function for binary cross entropy.\n",
    "\n",
    "        Args:\n",
    "         from_logits: True for logits, false for normalized log \n",
    "                probabilities (i.e., used sigmoid activation function).\n",
    "                Assumes not from logits.\n",
    "        \"\"\"\n",
    "\n",
    "        self.sigmoid = lambda t: 1 / (1 + np.exp(-t))\n",
    "        self.from_logits = from_logits\n",
    "\n",
    "    def gradient(self, inputs: tuple[np.ndarray, np.ndarray]) -> np.ndarray:\n",
    "        \"\"\"Derivative with respect to a single activation (same as derivative).\n",
    "\n",
    "        Should there be a from logits check here??\n",
    "\n",
    "        Args:\n",
    "            inputs: Targets, predictions vectors. Presumably, the inputs \n",
    "            here also have to be normalized log probabilities.\n",
    "\n",
    "        Returns:\n",
    "            Vector (gradient) of values.\n",
    "        \"\"\"\n",
    "        targets, predictions = inputs\n",
    "\n",
    "        if self.from_logits:\n",
    "            predictions = self.sigmoid(predictions)\n",
    "\n",
    "        return -1 * ((targets/predictions) - ((1-targets) / (1-predictions)))\n",
    "\n",
    "    def __call__(self,\n",
    "                 inputs: tuple[np.ndarray, np.ndarray],\n",
    "                 axis: Optional[int] = None) -> np.ndarray:\n",
    "        \"\"\"Compute cost given inputs.\n",
    "\n",
    "        Args:\n",
    "            inputs: Targets and predictions vectors. \n",
    "                Assumes predictions are not from logits.\n",
    "\n",
    "        Return:\n",
    "            Scalar cost.\n",
    "        \"\"\"\n",
    "\n",
    "        targets, predictions = inputs\n",
    "\n",
    "        if self.from_logits:\n",
    "            predictions = self.sigmoid(predictions)\n",
    "\n",
    "        return -1 * np.mean(targets * np.log(predictions) + (1 - targets) * np.log(1 - predictions), axis=axis)\n",
    "\n",
    "# MSE gradient example\n",
    "\n",
    "# Instantiate cost function objects\n",
    "mse = MeanSquaredError()\n",
    "bce = BinaryCrossEntropy()\n",
    "\n",
    "# Validate MSE grad\n",
    "a_L_np = np.array([0.12, 0.35, 0.61])\n",
    "y_true_np = np.array([0.11, 0.01, 0.59])\n",
    "a_L_tf = tf.Variable(a_L_np)\n",
    "y_true_tf = tf.constant(y_true_np)\n",
    "\n",
    "# tf gradient context\n",
    "with tf.GradientTape() as tape:\n",
    "    C = mean_squared_error(y_true=y_true_tf, y_pred=a_L_tf)\n",
    "\n",
    "dC_daL = tape.gradient(C, a_L_tf)\n",
    "print('-- MSE -- ')\n",
    "print('tf gradient tape:', dC_daL.numpy())\n",
    "\n",
    "# My implementation\n",
    "dC_daL_np = mse.gradient((y_true_np, a_L_np))\n",
    "print('mse.gradient:', dC_daL_np)\n",
    "print()\n",
    "\n",
    "#### BCE ####\n",
    "y_true = tf.constant(np.array([0., 1., 0., 0.]))\n",
    "y_pred_logits = np.array([-18.6, 0.51, 2.94, -12.8])\n",
    "y_pred_proba = tf.Variable(sigmoid(y_pred_logits))\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    C = binary_crossentropy(y_true, y_pred_proba)\n",
    "\n",
    "print('-- BCE --')\n",
    "dC_dProbaActivation = tape.gradient(C, y_pred_proba)\n",
    "print('tf gradient tape:', dC_dProbaActivation.numpy())\n",
    "dC_dProbaActivationMine = bce.gradient((y_true, y_pred_proba))\n",
    "print('bce.gradient:', dC_dProbaActivationMine.numpy())\n",
    "print('ml master deriv bce:', [deriv_bce(y_true[sample].numpy(), y_pred_proba[sample].numpy()) for sample in range(y_true.shape[0])])\n",
    "\n",
    "#### Outputs ####\n",
    "# -- MSE -- \n",
    "# tf gradient tape: [0.00666667 0.22666667 0.01333333]\n",
    "# mse.gradient: [0.00666667 0.22666667 0.01333333]\n",
    "\n",
    "# -- BCE --\n",
    "# tf gradient tape: [ 0.         -0.40012383  4.97895166  0.25000067]\n",
    "# bce.gradient: [ 1.00000001 -1.60049558 19.91584631  1.00000276]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tf gradient stock example\n",
    "# https://www.tensorflow.org/api_docs/python/tf/GradientTape\n",
    "# Here goes the neural network weights as tf.Variable\n",
    "x = tf.Variable([3.0, 5])\n",
    "\n",
    "def dydx(var):\n",
    "    \"\"\"y = x^{2}, dydx = 2x\"\"\"\n",
    "\n",
    "    return 2 * var\n",
    "\n",
    "# TensorFlow operations executed within the context of\n",
    "# a GradientTape are  recorded for differentiation \n",
    "with tf.GradientTape() as tape:\n",
    "  # Doing the computation in the context of the gradient tape\n",
    "  # For example computing loss\n",
    "  y = x ** 2 \n",
    "\n",
    "# Getting the gradient of network weights w.r.t. loss\n",
    "dy_dx = tape.gradient(y, x) \n",
    "my_dy_dx = dydx(x.numpy())\n",
    "print(dy_dx)  # Returns 6\n",
    "print(my_dy_dx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3)\n",
      "tf.Tensor([5043.25614252 5043.142572  ], shape=(2,), dtype=float64)\n",
      "[5043.25614252 5043.142572  ]\n",
      "\n",
      "tf.Tensor(\n",
      "[[  0.5844   0.     -82.    ]\n",
      " [  0.436    0.     -82.    ]], shape=(2, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[  0.5844   0.     -82.    ]\n",
      " [  0.436    0.     -82.    ]], shape=(2, 3), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "# MSE example\n",
    "y_true = tf.constant(np.array([[0.1234, 1., 123], [0.346, 0., 123]]))\n",
    "y_pred = tf.Variable(np.array([[1., 1., 0], [1., 0., 0]]))\n",
    "\n",
    "print(y_true.shape)\n",
    "\n",
    "mse = MeanSquaredError()\n",
    "print(mean_squared_error(y_true, y_pred))\n",
    "print(mse((y_true, y_pred), axis=-1))\n",
    "print()\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    C = mean_squared_error(y_true, y_pred)\n",
    "\n",
    "print(tape.gradient(C, y_pred))\n",
    "print(mse.gradient((y_true, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf gradient tape: [0.00666667 0.22666667 0.01333333]\n",
      "mse.gradient: [0.00666667 0.22666667 0.01333333]\n"
     ]
    }
   ],
   "source": [
    "# MSE gradient example\n",
    "\n",
    "# Instantiate mse object\n",
    "mse = MeanSquaredError()\n",
    "\n",
    "# Validate MSE grad\n",
    "a_L_np = np.array([0.12, 0.35, 0.61])\n",
    "y_true_np = np.array([0.11, 0.01, 0.59])\n",
    "a_L_tf = tf.Variable(a_L_np)\n",
    "y_true_tf = tf.constant(y_true_np)\n",
    "\n",
    "# tf gradient context\n",
    "with tf.GradientTape() as tape:\n",
    "    C = mean_squared_error(y_true=y_true_tf, y_pred=a_L_tf)\n",
    "\n",
    "dC_daL = tape.gradient(C, a_L_tf)\n",
    "print('tf gradient tape:', dC_daL.numpy())\n",
    "\n",
    "# My implementation\n",
    "dC_daL_np = mse.gradient((y_true_np, a_L_np))\n",
    "print('mse.gradient:', dC_daL_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean (axis=None) 0.25\n",
      "mean axis=-1: 0.25\n",
      "Inputs:\n",
      "y_true: [0 1 0 0]\n",
      "y_pred_logits: [-18.6    0.51   2.94 -12.8 ]\n",
      "y_pred_proba: [8.35839003e-09 6.24806474e-01 9.49788727e-01 2.76076495e-06]\n",
      "\n",
      "My implementation:\n",
      "bce(from_logits=False): 0.8654579497810978\n",
      "bce(from_logits=True): 0.8654579497810978\n",
      "\n",
      "Tensorflow:\n",
      "0.8654573847833843\n",
      "\n",
      "Random Calculations:\n",
      "TensorFlow: tf.Tensor(0.8897597402544937, shape=(), dtype=float64)\n",
      "My Implementation: 0.8897600640814369\n"
     ]
    }
   ],
   "source": [
    "# BCE example\n",
    "\n",
    "samples = 5\n",
    "dims = 4\n",
    "\n",
    "sigmoid = Sigmoid()\n",
    "\n",
    "y_true = np.array([0, 1, 0, 0])\n",
    "print('mean (axis=None)', np.mean(y_true, axis=None))\n",
    "print('mean axis=-1:', np.mean(y_true, axis=-1))\n",
    "y_pred_logits = np.array([-18.6, 0.51, 2.94, -12.8])\n",
    "y_pred_proba = sigmoid(y_pred_logits)\n",
    "\n",
    "random_true = np.random.choice(a=2, size=(samples, dims))\n",
    "random_proba = sigmoid(np.random.normal(size=(samples, dims)))\n",
    "\n",
    "print('Inputs:')\n",
    "print('y_true:', y_true)\n",
    "print('y_pred_logits:', y_pred_logits)\n",
    "print('y_pred_proba:', y_pred_proba)\n",
    "\n",
    "bce_fl = BinaryCrossEntropy(from_logits=True)\n",
    "bce = BinaryCrossEntropy()\n",
    "\n",
    "print()\n",
    "print('My implementation:')\n",
    "print('bce(from_logits=False):', bce((y_true, y_pred_proba)))\n",
    "print('bce(from_logits=True):', bce_fl((y_true, y_pred_logits)))\n",
    "\n",
    "print()\n",
    "print('Tensorflow:')\n",
    "print(binary_crossentropy(y_true, y_pred_proba).numpy())\n",
    "\n",
    "print()\n",
    "print('Random Calculations:')\n",
    "print('TensorFlow:', tf.reduce_mean(binary_crossentropy(y_true=random_true, y_pred=random_proba)))\n",
    "print('My Implementation:', bce((random_true, random_proba)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow:\n",
      "tf.Tensor([ 0.         -0.4016326   4.97895166  0.25000067], shape=(4,), dtype=float64)\n",
      "\n",
      "My Implementation:\n",
      "tf.Tensor([ 1.00000001 -1.60653066 19.91584631  1.00000276], shape=(4,), dtype=float64)\n",
      "[-2.]\n"
     ]
    }
   ],
   "source": [
    "# BCE gradient example\n",
    "\n",
    "samples = 5\n",
    "dims = 4\n",
    "\n",
    "sigmoid = Sigmoid()\n",
    "bce = BinaryCrossEntropy()\n",
    "\n",
    "y_true = tf.constant(np.array([0., 1., 0., 0.]))\n",
    "y_pred_logits = np.array([-18.6, 0.5, 2.94, -12.8])\n",
    "y_pred_proba = tf.Variable(sigmoid(y_pred_logits))\n",
    "y_pred_single_proba = np.array([0.5])\n",
    "y_true_single_response = np.array(1.)\n",
    "\n",
    "random_true = np.random.choice(a=2, size=(samples, dims))\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    C = binary_crossentropy(y_true, y_pred_proba)\n",
    "\n",
    "print('TensorFlow:')\n",
    "dC_dProbaActivation = tape.gradient(C, y_pred_proba)\n",
    "print(dC_dProbaActivation)\n",
    "print()\n",
    "\n",
    "print('My Implementation:')\n",
    "dC_dProbaActivationMine = bce.gradient((y_true, y_pred_proba))\n",
    "print(dC_dProbaActivationMine)\n",
    "\n",
    "print(bce.gradient((y_true_single_response, y_pred_single_proba)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[14  2 11  0]\n",
      " [14  3  5 12]\n",
      " [ 9 10  4 11]\n",
      " [ 4  6  4 15]] (4, 4)\n",
      "[[ 0.8644362 ]\n",
      " [-0.74216502]\n",
      " [ 2.26975462]\n",
      " [-1.45436567]\n",
      " [ 0.04575852]\n",
      " [-0.18718385]\n",
      " [ 1.53277921]\n",
      " [ 1.46935877]\n",
      " [ 0.15494743]\n",
      " [ 0.37816252]\n",
      " [-0.88778575]\n",
      " [-1.98079647]\n",
      " [-0.34791215]\n",
      " [ 0.15634897]\n",
      " [ 1.23029068]\n",
      " [ 1.20237985]] (16, 1)\n",
      "(4, 4, 1)\n",
      "\n",
      "[[ 1.23029068]\n",
      " [ 2.26975462]\n",
      " [-1.98079647]\n",
      " [ 0.8644362 ]]\n",
      "[[ 1.23029068]\n",
      " [-1.45436567]\n",
      " [-0.18718385]\n",
      " [-0.34791215]]\n",
      "[[ 0.37816252]\n",
      " [-0.88778575]\n",
      " [ 0.04575852]\n",
      " [-1.98079647]]\n",
      "[[0.04575852]\n",
      " [1.53277921]\n",
      " [0.04575852]\n",
      " [1.20237985]]\n"
     ]
    }
   ],
   "source": [
    "# X matrix\n",
    "batch_size = 4\n",
    "\n",
    "X = np.random.normal(size=(16, 1))\n",
    "\n",
    "num_samples = X.shape[0]\n",
    "num_batches = num_samples//batch_size\n",
    "batch_indices = np.random.choice(num_samples, size=(num_batches, batch_size))\n",
    "\n",
    "print(batch_indices, batch_indices.shape)\n",
    "print(X, X.shape)\n",
    "print(X[batch_indices].shape)\n",
    "print()\n",
    "\n",
    "for batch in X[batch_indices]:\n",
    "    print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = 1\n",
    "batch_size = 32\n",
    "A = np.random.normal(size=(batch_size, targets))\n",
    "y_true = np.random.normal(size=(batch_size, targets))\n",
    "Z = np.random.normal(size=(batch_size, targets))\n",
    "\n",
    "def compute_delta_L(a, y, z):\n",
    "    \"\"\"\"\"\"\n",
    "    return np.random.choice(size=(z.shape[0]))\n",
    "\n",
    "delta_L = np.apply_along_axis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (W^{l+1}_jk)^T DOT d_^{l+1}j HADAMARD phi(z_j)\n",
    "# C_k1 HADAMARD \n",
    "# j = num_neurons_in_cur_layer = 2\n",
    "# k = num_neurons_in_prev_layer = 3\n",
    "# o         o     \n",
    "#           o          o\n",
    "# o         o \n",
    "#       j=3, k=2    j=1, k=3\n",
    "# (W^{l+1}_13)^T DOT d^{l+1}_1 H phi(z_3)\n",
    "# W^{l+1}_31 DOT d^{l+1}_1 H phi(z_3)\n",
    "# C_3 H Phi(z_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 32)\n",
      "(1, 32)\n",
      "(32,)\n"
     ]
    }
   ],
   "source": [
    "a = np.random.normal(size=(32, 1))\n",
    "b = np.random.normal(size=(32, 1))\n",
    "c = np.random.normal(size=(32,))\n",
    "\n",
    "print(np.atleast_2d(c).shape)\n",
    "\n",
    "print(np.transpose(a).shape)\n",
    "print(np.transpose(c).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4]\n",
      "hadamard [0 2 4 6 8]\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "x = np.arange(0, 5)\n",
    "y = np.full_like(x, fill_value=2)\n",
    "\n",
    "print(x)\n",
    "print('hadamard', x * y)\n",
    "print(np.dot(x, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x25d665b6820>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.linspace(-5, 5)\n",
    "y = ReLU()(x)\n",
    "dydx = ReLU().derivative(x)\n",
    "plt.scatter(x, y)\n",
    "plt.scatter(x, dydx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input data\n",
    "rows = 5\n",
    "cols = 10\n",
    "input_vector = np.random.normal(size=(rows, cols))\n",
    "dummy_pred_vector = np.random.normal(size=(rows, cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate Layer\n",
    "lyr = DenseLayer(input_dims=input_vector.shape[-1], num_units=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: (5, 10)\n",
      "output: (5, 2) (5, 2)\n",
      "[[-0.42647349 -0.24779502]\n",
      " [-0.28689659 -0.40105034]\n",
      " [-0.32231351 -0.45992227]\n",
      " [-0.18448811  0.24800787]\n",
      " [ 0.17294692  0.21348298]] \n",
      "\n",
      " [[-0.42647349 -0.24779502]\n",
      " [-0.28689659 -0.40105034]\n",
      " [-0.32231351 -0.45992227]\n",
      " [-0.18448811  0.24800787]\n",
      " [ 0.17294692  0.21348298]]\n"
     ]
    }
   ],
   "source": [
    "# Layer call\n",
    "lyr_activations, lyr_wt_inputs = lyr(input_vector)\n",
    "print('input:', input_vector.shape)\n",
    "print('output:', lyr_activations.shape, lyr_wt_inputs.shape)\n",
    "print(lyr_activations, '\\n\\n', lyr_wt_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([], [])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MLP(input_dims=input_vector.shape[-1], hidden_units=2, targets=1, loss_function=mse, learning_rate=1e-3, l_layers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.13224191]\n",
      " [ 0.01783743]\n",
      " [-0.16562248]\n",
      " [-0.15245786]\n",
      " [-0.05169993]]\n",
      "(5, 1)\n"
     ]
    }
   ],
   "source": [
    "# Result of forward pass\n",
    "fwd = model._forward_pass(inputs=input_vector)\n",
    "print(fwd)\n",
    "print(fwd.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "[[22 51 90 17 94 76 89 87  8 53 98 55 50 31 26 28 88 79 34 74 39 85 12 16\n",
      "  83 64 20 67 58 77 25  7]\n",
      " [86 36 21 72 84 57  9 11 96 82 68 49  5 14 15 37 19 33 69 78 59 56  4 91\n",
      "  75  6 95 44 65 62 29 40]\n",
      " [71 24 81 41 73 45 13 23 42 52 92 38 63 60  2 46 70 93 32  3  0 80 97 99\n",
      "   1 30 48 27 47 43 18 10]]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "samples = 100\n",
    "y_batch_indices = np.random.choice(\n",
    "            a=samples, size=(samples//batch_size, batch_size), replace=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "07f0aceed467d0cbae7569fecdfaec78063cba8e23481668ef588a8de6eba6e0"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit ('blue-mars': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
